

\setcounter{chapter}{-1}
\chapter{Introduction}
\label{ch:introduction}


Human intelligence has the fascinating capability of recognizing musical instruments, rhythm, pitches and other structures in music. It can focus the auditory attention on a particular task and filter out a range of other stimuli. This part of our intelligence involves four key components: \textit{perception}, \textit{cognition}, \textit{knowledge representation} and \textit{inference} (\cite{benetos_automatic_2019}).
\textit{Perception} refers to the ability of analyzing input audio, and can significantly vary based on what it has learned over different occasions, unlike sensation, which remains relatively constant over time (\cite{obrien_epistemology_nodate}). \textit{Cognition} is the ability of recognizing musical objects, and can be seen as a resolution of a system (\cite{oppenheim_human_2013}). \textit{Knowledge representation} is the formation of musical structures from the obtained cognition, and \textit{inference} refers to the ability of learning from musical structures.


However, extracting musical structures with \textit{digital} equipment is a challenging task in signal processing and machine learning due to the complicated nature of music. Interference between sound waves, noise (unwanted sound), and reverberation can result in a complex cocktail of stimuli. Recent approaches in the literature have made several attempts to extract musical structures, including non-negative matrix factorization (NMF) (\cite{lopez-serrano_nmf_2019, holzapfel_musical_2008}), Bayesian approaches (\cite{donnelly_bayesian_2012, temperley_bayesian_2004}) and neural networks (\cite{draguns_residual_2021, sleep_automatic_2017}). Nonetheless, many aspects of musical analysis are still considered as open problems in the literature.


Although the power of (deep) neural networks should not be questioned, keeping the progress in the past years in mind, they do not \textit{really} simulate the functioning of the brain. Therefore, \textcite{wiggins_creativity_2020} proposed to construct an explanatory model with a level of abstraction that describes the hypothetical mechanisms (and not necessarily the effect) of the brain treating auditive signals. Since music analysis has a close relation to other signal processing problems, including speech recognition, the acquired solutions and insights throughout the process can help to solve similar problems from a cognitive perspective.


To advance towards the human-like \textbf{perception}, \textcite{homer_modelling_2023} proposed an idealized model of auditory receptive fields. The input information for this model are the so-called \textit{discrete resonances}, which are directly inspired from cochlear mechanics: the vibration of the eardrum in the outer ear can be described as a damped harmonic oscillator (\cite{chung_hearing_1981}), from which the middle ear translates pressure waves to mechanical energy. This awakens a resonance of the basilar membrane in the inner ear that can be described as a mechanical resonance. \textbf{Knowledge representation}, on the other hand, has been modeled by \textcite{harley_abstract_2020}. He developed a Common Hierarchical Abstract Knowledge Representation for Anything \parencite[CHAKRA]{harley_chakra_2022}. This type-based framework for knowledge
representation supporting the idea of life-long learning and can be used for the representation of musical knowledge (\cite{wiggins_representing_1989}).

The goal of this thesis is to tie the model of perception and knowledge representation together through a model for \textbf{cognition}. Although audio is in general expressive complete, containing a lot of information in a signal (unlike MIDI files), it lacks in structural generality, making it not evident to extract structures from audio files (\cite{wiggins_creativity_2020}).
We apply a clustering-based approach for the extraction of musical objects in a discrete resonance spectra and create a type-based knowledge representation specifically for audio signals. Our aim is to create a bidirectional system which scores well on both expressive completeness (e.g., audio waveforms) and structural generality (e.g., sheet music) (\cite{wiggins_representing_1989, collins_expressive_2018}). Finally, a demonstration will be given of the possibilities with the developed software by showcasing the extraction of pitch and overtones and how they are structured in a knowledge representation.

In the first part of this thesis, the reader will be guided through a few essential concepts in signal processing and psychoacoustics. These concepts play a crucial role in the understanding of our cognitive model and approach. Therefore, \textbf{Chapter 1} gives an introduction to the different categories in Fourier analysis and discusses the decomposition of a signal into complex exponentials applies to audio signals. \textbf{Chapter 2} delves deeper into this subject and provides a definition of the Fourier Transform within the context of a $L^2$ Hilbert Space. In \textbf{Chapter 3}, several aspects of psychoacoustics are briefly discussed, including the excessive explanation of our terminology (e.g., perception, constituent elements). 

The second part provides the theoretical background behind our state-of-the-art cognitive model.
\textbf{Chapter 4} lays out the fundamentals of cochlear mechanics and discusses several observations about the transfer of perceptual information between the basilar membrane in the cochlea and auditory cortex.  Using the presumption about cochlear mechanics, \textbf{Chapter 5} introduces the intricate world of discrete resonances.
We also define resonances in a Hilbert space, but this time, the basis in a Hilbert Space is not necessarily orthogonal anymore. This entails interesting features for musical analysis, including precision. This level of precision will play a crucial role in grouping the resonances. Therefore, \textbf{Chapter 6} describes the background behind a density-based clustering algorithm \parencite[DBSCAN]{ester_density-based_1996}. This method will be applied to the resonance spectra for the generation of a stronger structural generality in audio files. The acquired knowledge will be structured in a type-based knowledge representation. \textbf{Chapter 7} finally presents the theory behind the framework \parencite[CHAKRA]{harley_abstract_2020}.


The last part outlines my own contribution to this research field. \textbf{Chapter 8} describes our particular implementation of the CHAKRA framework, and \textbf{Chapter 9} discusses the implementation of our model that simulates cognition in human intelligence. We simulate the connections made in the brain to perceive music through a density-based clustering approach. We use the DBSCAN algorithm, which clusters data as a human would do, and compare the performance of two hyperparameter estimations, namely the silhouette score and the \textit{kneedle} method. Finally, we demonstrate the performance of the cognition of pitch and overtones.